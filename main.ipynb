{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Get Source Data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nest_asyncio\n",
    "nest_asyncio.apply()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import openai\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "from scrapegraphai.graphs import SmartScraperGraph\n",
    "from scrapegraphai.utils import prettify_exec_info\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "openai_key = os.getenv(\"OPENAI_API_KEY\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "graph_config = {\n",
    "   \"llm\": {\n",
    "      \"api_key\": openai_key,\n",
    "      \"model\": \"openai/gpt-3.5-turbo\", # specify model here\n",
    "   },\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "smart_scraper_graph = SmartScraperGraph(\n",
    "   prompt=\"\"\"Extract all links from the given documentation page.\n",
    "               \"\"\",\n",
    "   source=\"-------------- Your link of the site to fetch data from ---------------\",\n",
    "   config=graph_config\n",
    ")\n",
    "\n",
    "result = smart_scraper_graph.run()\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_url = \"-----------Add base link to combine with extracted links------------\"  \n",
    "\n",
    "full_links = [base_url + link for link in result['links']]  \n",
    "\n",
    "for full_link in full_links:  \n",
    "    print(full_link)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_links = [link for link in full_links if link.count(\"https\") ==1]  \n",
    "len(filtered_links)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def json_to_text(data, indent=0):\n",
    "    \"\"\"Recursively converts JSON data to plain text.\"\"\"\n",
    "    result = ''\n",
    "    indent_str = ' ' * indent\n",
    "\n",
    "    if isinstance(data, dict):\n",
    "        for key, value in data.items():\n",
    "            result += f\"{indent_str}{key}:\\n\"\n",
    "            result += json_to_text(value, indent + 2)\n",
    "    elif isinstance(data, list):\n",
    "        for index, item in enumerate(data):\n",
    "            result += f\"{indent_str}- Item {index + 1}:\\n\"\n",
    "            result += json_to_text(item, indent + 2)\n",
    "    else:\n",
    "        result += f\"{indent_str}{data}\\n\"\n",
    "\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.docstore.document import Document\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "\n",
    "docs = []\n",
    "\n",
    "for link in filtered_links:\n",
    "    smart_scraper_graph = SmartScraperGraph(\n",
    "        prompt=\"\"\"Extract all key information including headings with paragraphs, lists, source codes and any relevant data points from the provided link.\n",
    "                \"\"\",\n",
    "        source=link,\n",
    "        config=graph_config\n",
    "    )\n",
    "\n",
    "    out = smart_scraper_graph.run()\n",
    "    convert = json_to_text(out)\n",
    "    print(convert, '\\n\\n')\n",
    "\n",
    "    docs.append(Document(page_content=convert))\n",
    "    print(f'\\niterated on link {link}\\n\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Splitting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "\n",
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=4000, chunk_overlap=200, add_start_index=True\n",
    ")\n",
    "\n",
    "all_splits = text_splitter.split_documents(docs)\n",
    "\n",
    "print(len(all_splits))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Storing in Vector Store"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_chroma import Chroma\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "\n",
    "vectorstore = Chroma.from_documents(documents=all_splits, embedding=OpenAIEmbeddings())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Specifying Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "llm = ChatOpenAI(model=\"gpt-4o-mini\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Generate Data Using RAG:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('--------- Your Data Frame Containing Prompts -----------')\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_docs(docs):\n",
    "    return \"\\n\\n\".join(doc.page_content for doc in docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.prompts import PromptTemplate\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.runnables import RunnablePassthrough\n",
    "\n",
    "reses = []\n",
    "\n",
    "for i, row in df.iterrows():\n",
    "\n",
    "    question = row['prompt']\n",
    "\n",
    "    retriever = vectorstore.as_retriever(search_type=\"similarity\", search_kwargs={\"k\": 6})\n",
    "\n",
    "    retrieved_docs = retriever.invoke(question)\n",
    "\n",
    "    len(retrieved_docs)\n",
    "\n",
    "    template = \"\"\"\n",
    "        ---------------- Your Prompt Here For Generating Data ---------------\n",
    "    \"\"\"\n",
    "    custom_rag_prompt = PromptTemplate.from_template(template)\n",
    "\n",
    "    rag_chain = (\n",
    "        {\"context\": retriever | format_docs, \"question\": RunnablePassthrough()}\n",
    "        | custom_rag_prompt\n",
    "        | llm\n",
    "        | StrOutputParser()\n",
    "    )\n",
    "\n",
    "    response = rag_chain.invoke(question)\n",
    "    readable_response = response.replace('\\\\n', '\\n').replace('\\\\t', '\\t')\n",
    "    reses.append({'prompt': question, 'responses': resp})\n",
    "\n",
    "    print(resp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Dump Generated Data into DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.DataFrame(reses)\n",
    "data.to_csv('data.csv', index = False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
